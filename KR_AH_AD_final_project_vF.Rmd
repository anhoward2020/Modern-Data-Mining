---
title: "Text Mining"
author: "Modern Data Mining"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, results = "hide", fig.width=8, fig.height=4)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, ggplot2, tm, RColorBrewer, wordcloud, glmnet,
               randomForest, ranger, data.table)
```
\pagebreak

```{r read dataq, echo=F}
data <- read.csv("AB_NYC_2019.csv", stringsAsFactors = T)
```

```{r data cleaningq, echo=F}
data <- data[,-c(1,2,3,4,7,8,13)]
days_per_year = 365
data <- data %>% 
  mutate(booked = days_per_year - data$availability_365)

data <- data %>% 
  mutate(profit = data$booked * data$price)

colnames(data)
```

```{r check for missing values, echo=F}
data <- na.omit(data)
sum(is.na(data))
```


# Characterization of the Response Variables

```{r EDA, echo=F}
#logprofit creation
data$logprofit <- log(data$profit)

#basic statistical summaries
mean(data$booked)
median(data$booked)
hist(data$booked, breaks=20)

#dist of response variables
data %>% select(booked) %>% 
  summarise(
    mean = mean(booked),
    sd   = sd(booked),
    max = max(booked),
    "0%" = quantile(booked)[1],
    "25%" = quantile(booked)[2],
    "50%" = quantile(booked)[3],
    "75%" = quantile(booked)[4],
    "100%" = quantile(booked)[5]
  )

#Actually a solid normal dist on the log scale
median(data$profit)
max(data$profit)
max(data$price)
hist(data$logprofit, breaks=20)

#response variable summaries by neighborhood
data %>% group_by(neighbourhood_group) %>%
  summarise(mean(booked), median(booked))

#transformed back for sensibility in this case
data %>% group_by(neighbourhood_group) %>%
  summarise(mean(profit), median(profit)) 

plot(data$booked ~ data$neighbourhood_group)
plot(data$profit ~ data$neighbourhood_group)

#Basic neighborhood information
data %>% group_by(neighbourhood) %>%
  summarize(minVal = min(profit), minValName = neighbourhood[which.min(profit)], 
            maxVal = max(profit), maxValName = neighbourhood[which.max(profit)],
            avgVal = mean(profit, avgValName = neighbourhood[mean(profit)]))


```

# Plots for Categorical Predicting Variables

```{r categorical predictors, echo=F}

#Correlation tables to see which predictors to examine
data %>% select_if(is.numeric) %>%
  cor() # pairwise cor's among all quantitative var's

#Correlation for selected predictors in heatmap
corr.table <- data %>% select_if(is.numeric) %>% 
  select(minimum_nights, number_of_reviews, reviews_per_month, calculated_host_listings_count, booked, profit) %>%
  cor() %>% melt()

corr.table %>%
ggplot(aes(x=Var1, y=Var2, fill=value)) +
  geom_tile() +
  xlab("") +
  ylab("") +
  guides(fill = guide_legend(title = "Correlation")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_x_discrete(limits = rev(levels(corr.table$Var1))) +
  scale_fill_gradient(low = "#56B1F7", high = "#132B43") # from lightblue to darblue
```

```{R}
#Clear positive relationship between response variables
ggplot(data, aes(x=log(profit), y=booked, shape=neighbourhood_group, color=neighbourhood_group)) +
  geom_point()

ggplot(data, aes(x=minimum_nights, y=calculated_host_listings_count, shape=neighbourhood_group, color=neighbourhood_group)) +
  geom_point()

ggplot(data, aes(x=reviews_per_month, y=number_of_reviews, shape=neighbourhood_group, color=neighbourhood_group)) +
  geom_point()

```


```{r split into training and testing, echo=F}
#first need to remove availability and profit for linear model
colnames(data)
data <- data[,-c(9,11,12)]

#split
idx <- sample(seq(1, 2), size = nrow(data), replace = TRUE, prob = c(.8, .2))

data.train <- data[idx == 1,]
data.test <- data[idx == 2,]
```


```{r linear backwards selection, echo=F}
library(car)
backwards.fit1 <- lm(booked ~ ., data=data.train)
Anova(backwards.fit1)

#remove neighbourhood_group for interpretability
backwards.fit2 <- lm(booked ~ room_type + price + minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count, data=data.train)
Anova(backwards.fit2)

#all variables significant at 0.001 level

```

```{r LASSO, echo=F}
colnames(data.train)
Y <- data.train[,9] # extract Y
X <- model.matrix(booked~., data=data.train)[, -1]

#Fit LASSO using alpha = .99 with the goal of a parsimonious model
LASSO.fit <- cv.glmnet(X, Y, alpha=.99, nfolds=10)  
plot.mse.99 <- plot(LASSO.fit) #use lamba 1se for more parsimonious model

#run lambda 1se
LASSO.fit$lambda.1se 
    fit.lambda.99.1se <- glmnet(X, Y, alpha=.99, lambda = LASSO.fit$lambda.1se)
    fit.lambda.99.1se$df

#extract variables for relaxed lasso
coef.lasso <- coef(LASSO.fit, s="lambda.1se") 
coef.lasso <- coef.lasso[which(coef.lasso !=0),]  
coef.lasso

#relaxed lasso - variables: host_id, neighbourhood_group, neighbourhood, room_type, price, minimum_nights, number_of_reviews, reviews_per_month, calculated_host_listings_count
relaxed.lasso <- lm(booked ~ neighbourhood_group + neighbourhood + room_type + price, minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count, data=data.train)
Anova(relaxed.lasso)


#remove neighbourhood_grou
relaxed.lasso3 <- lm(booked ~ neighbourhood + room_type + price, minimum_nights + number_of_reviews + reviews_per_month + calculated_host_listings_count, data=data.train)
summary(relaxed.lasso3)
Anova(relaxed.lasso3)

#minimum_nights, number_of_reviews, reviews_per_month, minimum_nights, and calculated_host_listings_count don't even show up in the Anova, so I chose to remove them

relaxed.lasso4 <- lm(booked ~ neighbourhood + room_type + price, data=data.train)
Anova(relaxed.lasso4)

#all variables significant

```


```{r full tree, echo=FALSE}
library(rpart.plot)
library(partykit)
#remove neighborhood and group for tree to increase interpretability
data.train.tree <- data.train[,-c(1,2)]

#fit tree
tree.fit <- rpart(booked ~., data.train.tree, minsplit=20, cp=.009)
plot(as.party(tree.fit), main="Final Tree with Rpart")
```

```{r bootstrap and bag two trees, echo=F}
#bootstrap
train.b1 <- sample(data.train.tree, 10830, replace=TRUE)
train.b2 <- sample(data.train.tree, 10830, replace=TRUE)

n <- nrow(data.train.tree)
b1.index <- sample(n, 10208, replace = TRUE) 
train.b1  <- data.train.tree[b1.index,] 
b2.index <- sample(n, 10208, replace = TRUE) 
train.b2  <- data.train.tree[b2.index,]

#assign trees
tree.fit2.1 <- rpart(booked ~., train.b1, minsplit=20, cp=.009)
tree.fit2.2 <- rpart(booked ~., train.b2, minsplit=20, cp=.009)
```

```{r, echo = F, fig.width= 7, fig.height= 7, fig.align="center"}
plot(tree.fit2.1) 
text(tree.fit2.1)
```
   
```{r, echo = F, fig.width= 7, fig.height= 7, fig.align="center"}
plot(tree.fit2.2) 
text(tree.fit2.2)
``` 


```{r random forest, echo=F}

#fit RF
#set.seed(1) #to control randomness and reproducibility
#fit.randomforest <- randomForest(booked~., data.train.tree, mtry= 3, ntree=250)
```

```{r OOB error, echo=F}

#will change when we have the appropriate data split

#OOB Error
#fit.randomforest.final$mse
#Testing Error
#fit.rf.testing <- randomForest(booked~., data.train.tree, xtest=data.test[, -length(data.test)],
#                          ytest=data.test[,length(data.test)], mtry=3, ntree=250)

#Testing error at ntree = 250, which we used in final model 
#randomforest.testingerror <- fit.rf.testing$mse[250] #0.7608693

#plot(1:250, fit.randomforest$mse, col="red", pch=16, cex = .7,
#     xlab="Number of Trees",
#     ylab="MSE",
#     main="Random Forest OOB and Testing Errors")
#points(1:250, fit.rf.testing$mse, col="blue", pch=16, cex = .7)
#legend("topright", legend=c("OOB Errors", "Testing Errors"), col=c("red", "blue"), lty=1, cex=1)
```

```{r read data, echo=F}
word <- read.csv("AB_NYC_2019.csv", stringsAsFactors = T)
```

```{r data cleaning, echo=F}
colnames(word)
word <- word[,-c(1,3,4,7,8,13)]
days_per_year = 365
word <- word %>% 
  mutate(booked = days_per_year - word$availability_365)

word <- word %>% 
  mutate(profit = word$booked * word$price)

```


i. Document term matrix (dtm)
```{r dtm, echo=F}
text_data <- word$name
dfCorpus <- VCorpus(VectorSource(text_data))
# 1. Stripping any extra white space:
dfCorpus <- tm_map(dfCorpus, content_transformer(stripWhitespace))
# 2. Transforming everything to lowercase
dfCorpus <- tm_map(dfCorpus, content_transformer(tolower))
# 4. Removing punctuation
dfCorpus <- tm_map(dfCorpus, content_transformer(removePunctuation))
#5. remove some words
# 5. Removing stop words
dfCorpus <- tm_map(dfCorpus, content_transformer(removeWords), stopwords("english"))
# 6. Stem document
#dfCorpus <- tm_map(dfCorpus, content_transformer(stemDocument), lazy = TRUE)   

dtm1 <- DocumentTermMatrix(dfCorpus)
inspect(dtm1)
```

ii. remove sparse terms 
```{r Reduce size of bag, echo = F}
# `removeSparseTerms()`: keep words that appear at least once in X% of documents.
dtm <- removeSparseTerms(dtm1, 1 -.001)  # sparsity < .99 
#dim(as.matrix(dtm))  
inspect(dtm)
```

iii. create new variable "booked_half": airbnbs that are booked half of the year have value 1, otherwise 0
```{r create new variable, echo = F}
word$booked_half <- c(0)
word$booked_half[word$booked >= 273] <- 1
word$booked_half <- as.factor(word$booked_half)

data2 <- data.frame(word$booked_half, as.matrix(dtm))  
```

iv. split in testing, training, and validation
```{r split data, echo = F}
#str(data2)

set.seed(1)  # for the purpose of reproducibility
n <- nrow(data2)
training.index <- sample(n, 27000) 
data2.train <- data2[training.index,] #get training data with 1300 reviews
data2.remaining <- data2[-training.index, ]

n <- nrow(data2.remaining)
test.index <- sample(n, 7000)
data2.test <- data2.remaining[test.index, ]  #get testing data with 5000 reviews 
data2.validation<- data2.remaining[-test.index,] #get validation data with remaining reviews 

#check our dimensions are correct
str(data2.train)
#dim(data2.test)
dim(data2.validation)
```

v. fit lasso model
```{r LASSO fit, echo = F}
y <- data2.train$word.booked_half
X <- data.matrix(data2.train[, -c(1)]) # we can use as.matrix directly here
set.seed(2)

result.lasso <- cv.glmnet(X, y, alpha=.99, family="binomial") 
plot(result.lasso)
```

```{r get LASSO estimates, echo = F}
beta.lasso <- coef(result.lasso, s="lambda.1se")   # output lasso estimates
beta <- beta.lasso[which(beta.lasso !=0),] # non zero beta's
beta <- as.matrix(beta);
beta <- rownames(beta)
```

vi. logistic regression from lasso
```{r prepare glm input, echo = F}
glm.input <- as.formula(paste("word.booked_half", "~", paste(beta[-1],collapse = "+"))) # prepare the formulae
result.glm <- glm(glm.input, family=binomial, data2.train)
```	

vii. positive bag of words
```{r positive bag, echo = F}
result.glm.coef <- coef(result.glm)
summary(result.glm)
hist(result.glm.coef, main = "Histogram of glm Coefficients", xlab = "glm Coefficients") 

# pick up the positive coef's which are positively related to the prob of being a good review
good.glm <- result.glm.coef[which(result.glm.coef > 0)]
good.glm <- good.glm[-1]  # take intercept out
#length(good.glm) 
good.fre <- sort(good.glm, decreasing = TRUE) # sort the coef's
leading2.pos <- good.fre[1:2]  # which words are positively associated with good ratings
```

viii. positive word cloud
```{r results=TRUE, warning=FALSE, message=FALSE, echo = F, fig.align = "center"}
cor.special <- brewer.pal(8,"Spectral")  # set up a pretty color scheme
good.word <- names(good.fre)  # good words with a decreasing order in the coeff's
set.seed(1)
wordcloud(good.word[1:97], good.fre[1:97],  # make a word cloud
          colors=cor.special, ordered.colors=F)
```

ix. negative bag of words
```{r negative bag, echo = F}
# pick up the positive coef's which are positively related to the prob of being a good review
bad.glm <- result.glm.coef[which(result.glm.coef < 0)]
bad.glm <- bad.glm[-1]  # take intercept out
bad.fre <- sort(bad.glm, decreasing = F) # sort the coef's
leading2 <- bad.fre[1:2]  # which words are positively associated with good ratings

leading2 <- round(leading2, 3)
leading2 <- as.data.frame(leading2)
```

x. negative word cloud
```{r negative cloud, warning=FALSE, echo = F, fig.align = "center"}
cor.special <- brewer.pal(8,"RdBu")  # set up a pretty color scheme
bad.fre <- sort(-bad.glm, decreasing = TRUE)
bad.word <- names(bad.fre) 
set.seed(1)
wordcloud(bad.word[1:82], bad.fre[1:82],  # make a word cloud
          colors=cor.special, ordered.colors=F, min.freq = .4)
```

xi. assess how lasso model does using lambda.1se for classification
```{r assess, include= F}
# output majority vote labels
predict.lasso <- predict(result.lasso, as.matrix(data2.test[, -1]), type = "class", s="lambda.1se")
  
# LASSO testing errors
testerror.lasso <- mean(data2.test$data.booked_half != predict.lasso)   # 0.136

# output lasso estimates of prob's
predict.lasso.p <- predict(result.lasso, as.matrix(data2.test[, -1]), type = "response", s="lambda.1se")

# ROC curve for LASSO estimates
roc.lasso <- pROC::roc(data2.test$word.booked_half, predict.lasso.p, plot=TRUE)
lasso.auc <- round(roc.lasso$auc,4)
```

xii. assess how glm model does for classification
```{r assess part 2, include = F}
predict.glm <- predict(result.glm, data2.test, type = "response")
class.glm <- rep("0", 10000)
class.glm[predict.glm > .5] ="1"

#GLM testing errors
testerror.glm <- mean(data2.test$word.booked_half != class.glm)

# ROC curve for GLM estimates
roc.glm <- pROC::roc(data2.test$word.booked_half, predict.glm, plot=T) 
glm.auc <- round(roc.glm$auc, 4)
```

xiii. comapre glm and lasso testing error and AUC 
```{r comparison, echo = F}
library(kableExtra)
glm <- c(testerror.glm, glm.auc)
lasso <- c(testerror.lasso, lasso.auc)
eval <- rbind(lasso, glm)

eval <- round(eval, 4)
eval <- as.data.frame(eval)
colnames(eval) <- c("Testing Error", "AUC")
rownames(eval) <- c("LASSO", "glm")
kable(eval)  %>%   kable_styling(bootstrap_options = "striped", full_width = F)
```